
#Stage 1
In this stage, the user should have previous knowledge of the biogeochemical cycle or metabolic pathway to analyze, in this case we provide all the data involved in the sulfur cycle 

###Required datasets provided by the user 

1) A multifasta file of protein coding genes involved in a specific biogeochemical cycle, or several metabolic pathways (i.e C,N,S,cycles or hidrocarbon degradation). In our case we provided the example for S-cycle

```{r,eval=FALSE}
less input_data/sucy_database_uniprot.fasta
```

2) A comprhensive list of curated organisms. In our case we provided the list of sulfur energy based prokaryotes (See also Supplementary Table S1 )

```{r,eval=FALSE}

#a) Genearate a list of genus of interest 
head input_data/genus_suli.txt 

Desulfovibrio 
Desulfotomaculum
Beggiatoa 

#b) Check if those genus have a complete sequenced genome in the assembly file from refseq: See Non redundant dataset"

for i in `cat genus_suli.txt` ; do grep $i assembly_refseq.nr2016.txt |cut -f 8  >> suli.nr21122016.txt ; done

less input_data/suli.nr21122016.txt
```

3) Omic datasets. Genomic and Metagenomic data are neccesary to run the algorithm.  

###Genomic dataset (Old data 2014)

```{r,eval=FALSE}

#Files located in /data/old_data2014
#Dowload the complete  annotated genomes from NCBI using the following command. Ours were retrieved on June 25 2014:  
#wget ftp://ftp.ncbi.nlm.nih.gov/genomes/Bacteria/all.faa.tar.gz
#Since the FTP directory changed this command now should be updated to
wget ftp://ftp.ncbi.nih.gov/genomes/archive/old_refseq/Bacteria/all.faa.tar.gz
tar xvzf all.faa.tar.gz
# At this point should be  downloaded 2775 genomes
ls | wc 
```

###Non-redundant Genomic dataset

Go to the [algorithm](http://microbiome.wlu.ca/research/redundancy/redundancy.cgi) described in [Moreno et al., 2012](http://bioinformatics.oxfordjournals.org/content/early/2013/02/27/bioinformatics.btt064.full), and Choose a GSS value.

We chose the following parameters:

```{r,eval=FALSE}

#Files located in /data/old_data2014
#GSS / DNA Signature = GSSb
#GSS threshold = 0.95
#DNA-signature threshold = 0.01
#Sort by size or overannotation = largest
#Results style = simple list
#and save it to a local file 
less  data/list_nr_genomes_24042014.txt  
```

###Generate a FASTA file with peptide sequences from non-redundant genomes (old data 2014)

```{r,eval=FALSE}
#Files located in /data/old_data2014
#For this task we require the script  add_nr_genomes.pl located in the scripts directory 
#The script takes as input i) the uid list generated above and ii) the directory of the genomic dataset 
#This command  will generate a multifastafile of 1528 genomes  
perl /scripts/add_nr_genomes.pl data/list_nr_genomes_24042014.txt ncbi_genomes/ > data/GENOMES_NCBI_nr_24042014.faa
```


###Current realease of Refseq genomes 20  dic 2016 
##Non redundant dataset 
Make sure you have at least of 109.3 G of free space in your computer to store the genomic and genomic fragmented dataset 

```{r,eval=FALSE}
#Personal communication with Gabriel Moreno.He provided us with the  GSSb List, using the above parameters but with the current database of complete genomes 4085 in total 
less /data/genomic_dataset/GSSb-0.95.txt

#Parse the list in order to obtain the identifiers of non-redundant genomes
cd /data/genomic_dataset
sed 's/,/\t/g' GSSb-0.95.txt  | sed 's/ /\t/g'  | cut -f 3  > list_nr_genomes_21122016.txt

#Obtain the assembly data from those genomes 

wget "ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/assembly_summary_refseq.txt" 

for i in `cat list_nr_genomes_21122016.tx` ; do grep $i assembly_summary_refseq.txt >> assembly_refseq.nr2016.txt ; done

#Get the download links 

less assembly_refseq.nr2016.txt | cut -f 20 | sed 's/$/\/*.faa.gz/g' > assembly_refseq.nr2016.download.txt

#Download the genomes in faa format  

wget -i assembly_refseq.nr2016.download.txt

gunzip *.gz 

```

###IMPORTANT step changing the headers !!

Before generate a multifasta file, it is important to change the headers of all the genomes for latter purposses (Stage 2) 

```{r,eval=FALSE}

for i in *.faa ; do perl -lne 'if(/^>(\S+)/){ print ">$1 [$ARGV]"} else{ print }' $i > $i.named.faa; done

```

```
#This will change the header, from this  

>WP_003320558.1 MULTISPECIES: ATP-binding protein [Bacillus]
MNEQIQAYAKRLKLSWIRENFNQIEAETNEEYLLKLFEKEVQNREERKVNLLLSQAQLPKTGSTPFQWEHIQIPQGIERT

#To this 
>WP_003320558.1 [GCF_000005825.2_ASM582v2_protein.faa]
MNEQIQAYAKRLKLSWIRENFNQIEAETNEEYLLKLFEKEVQNREERKVNLLLSQAQLPKTGSTPFQWEHIQIPQGIERT
```

###Once the headers of all non-redundant genomes are changed, geenerate a multifastafile 

```{r,eval=FALSE}

cat *.named.faa > genomes_refseq_nr_22122016.faa

```

###Generate one line multifasta file using the following perl lne comand

```{r,eval=FALSE}
#old data
#perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (sort(keys(%fa))){ print "$s\n$fa{$s}\n" }}' GENOMAS_NCBI_nr_24042014.faa > GENOMAS_NCBI_nr_24042014.1.line.faa

perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (sort(keys(%fa))){ print "$s\n$fa{$s}\n" }}' genomes_refseq_nr_22122016.faa > genomes_refseq_nr_22122016.1.faa & 

```


###Non redundant-genomic fragmented dataset  

Fragment the genomic dataset using the script get_protein_fragments.pl

```
Program to produce random fragments of proteins in input file with size and coverage set by user.
usage: get_protein_fragments.pl [options] 
 -help             brief help message
  -inFASTA          input file with protein sequences in FASTA format
 -outFASTA         output file with protein fragments in FASTA format
  -size             desired size for produced random fragments    (integer, default 100)
  -cover            desired protein coverage of produced fragment (integer, default 1) 
```

```{r,eval=FALSE}
#Run each time depending on the desired sizes (30,60,100,150,200,250,300) make sure you are changing the output names
perl /scripts/get_protein_fragments.pl -inFASTA genomes_refseq_nr_21122016.1.faa -oUTFASTA genomes_refseq_nr_21122016_size30_cover10.faa -size 30 -cover 10 

mv **cover* genogenomic_dataset_fragmented
```

Using the above commands you should have the folowing files:

| Details          | File name                                      | Size File |
|------------------|------------------------------------------------|-----------|
| nr-genomes | genomes_refseq_nr_22122016.1.faa               | 2.7G      |
| nr-genomes size 30          | genomes_refseq_nr_21122016_size30_cover10.faa  | 7.8 G     |
| nr-genomes size 60          | genomes_refseq_nr_21122016_size60_cover10.faa  | 9.8 G     |
| nr-genomes size 100         | genomes_refseq_nr_21122016_size100_cover10.faa | 13 G      |
| nr-genomes size 150         | genomes_refseq_nr_21122016_size150_cover10.faa | 16 G      |
| nr-genomes size 200         | genomes_refseq_nr_21122016_size200_cover10.faa | 18 G      |
| nr-genomes size 250         | genomes_refseq_nr_21122016_size200_cover10.faa | 20 G      |
| nr-genomessize 300         | genomes_refseq_nr_21122016_size200_cover10.faa | 22 G      |



###Metagenomic dataset 
Generate a list of metagenomes of interest. We selected only those metagenomes from the metagenomics RAST server (mg-RAST) version 3.6 that meet the following conditions:

1. Public and available metagenomes

2. Available metadata associated 

3. Environmental samples (isolated from defined environments o features: rivers, soil, biofilms, etc), discarding the microbiome associated metagenomes (i.e human, cow, chicken)

4. We also included 35 private metagenomes unpublished derived from sediment, water and microbial mats from Cuatro Cienegas Coahuila (De Anda, in progress)
Using the above mentioned conditions, a total of 935 id-numbers were saved in list format

```{r,eval=FALSE}
cd data/metagenomic_dataset
less data/id_metagenomes.txt

```

5. Obtain the corresponding encoded protein sequences using the API from MG-RAST, were ech number corresponded to one of the processing steps 
```
.050.1 ==> upload.fna                 
.100.1 ==> preprocess.passed.fna      
.100.1 ==> preprocess.removed.fna     
.150.1 ==> dereplication.passed.fna   
.150.1 ==> dereplication.removed.fna  
.299.1 ==> screen.passed.fna          
.350.1 ==> genecalling.faa      #We use this stage to dowload protein coding genes  
.425.1 ==> rna.filter.fna             
.440.1 ==> cluster.rna97.mapping      
.440.1 ==> cluster.rna97.fna          
.450.1 ==> rna.sims                   
.550.1 ==> cluster.aa90.mapping       
.550.1 ==> cluster.aa90.faa           
.650.1 ==> protein.sims               
.700.1 ==> annotation.sims.filter.seq 
```
```{r, eval=FALSE}
#Begin downloading the files, this will take several hours . Make sure you have enough space  (at least 500 Gb) on your computer, and you are located in metagenomic_dataset 

for line in  `cat id_metagenomes.txt` ; do wget "http://api.metagenomics.anl.gov/1/download/mgm$line?file=350.1" -O $line ; done

#The dowloading files should look like this:

>FA4SSCL02FZQ74_1_237_-
LGRPIDIESLDVSFWGGLGVVLGNVVISNPEDMPGDTLMVAKEIDVKLQLWPLLSSEVRADRFIINDPTIRLHKTADG
>FA4SSCL02ILJM5_1_200_-
YIYLTLVEFKGQAAGLRERESIQHFQWSDKQVSKKEGSFGVDHIWYLQGIFMCSSIQKDYPKIK
>FA4SSCL02ISFX8_1_222_+
DEEAMHYDADYVRALEYGMPPTAGEGIGIDRLVMLLTDSPSIRDVLLFPHLRSEKGRQSSVHTFSMKLIFA

#Make sure that all the files were dowloaded correctly 
find -empty | wc


```

### Mean Size Lenght  (MSL) 
Taking into account the lenght size variation in the input proteins, their detection in the metagenomic samples will likely be affected by the average length of the reads. For example, we reasoned that the detection of proteins with more than 500 aa would be challenging in metagenomes with average read length of 300 aa. In addition, larger proteins, are likely to have more domains (i.e catalytic, cofactor binding etc) of variable sizes. Consequently, we expected that the detection of certain protein families would be severely affected in small metagenomic reads (less than 20 amino acids in mean size). Therefore, we compute the mean size lenght of ech metagenome using the following script:
S
```{r,eval=FALSE}

for FILE in *; do perl -lne 'if(/^(>.*)/){$h=$1}else{$fa{$h}.=$_} END{ foreach $h (keys(%fa)){$m+=length($fa{$h})}; printf("%1.0f\t",$m/scalar(keys(%fa))) }' $FILE; echo $FILE; done > MSL.tab

#We have added the example of the MSL output from our privates 35 metagenomes  
less data/metagenomic_dataset/MSL.privates.tab
```

```{r,eval=FALSE}
#Plot the histogram open R terminal 
data<-read.table("MSL.privates.tab", header=F, sep="\t")
pdf("hist.privates.pdf")
hist(data$V1,main= "MSL private metagenomes",xlab= "MSL in aa")
dev.off()
quit()
evince hist.privates.pdf  & 

```



![Histogram](data/metagenomic_dataset/hist.privates.pdf)



